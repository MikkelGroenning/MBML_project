{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wnTueG1WtMPn"
      },
      "source": [
        "# Week 08 - Topic Modeling - Latent Dirichlet Allocation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IOgLVoQptMPq"
      },
      "source": [
        "------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rm5kV7jqkq6P",
        "outputId": "b0107b9f-ce74-4680-abc8-ed2667cbaf69",
        "colab": {}
      },
      "source": [
        "from IPython.display import HTML\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/DTl5Xetaprw\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/Users/rodr/env37/lib/python3.7/site-packages/IPython/core/display.py:689: UserWarning: Consider using IPython.display.IFrame instead\n",
            "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/DTl5Xetaprw\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bNs5OB4ctMPr"
      },
      "source": [
        "Welcome back! Today, we'll work on a very useful text analysis technique called Topic Modeling. Particularly, we will use its most popular and powerful algorithm: Latent Dirichlet Allocation, or LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OoXZiqOjtMPs"
      },
      "source": [
        "Before you implement your own LDA on Pyro, it is important that you understand all concepts well. In this notebook, we'll start by playing a little bit with the Dirichlet distribution. After this, we'll ask you to do some ancestral sampling on the LDA generative story.\n",
        "\n",
        "Only after that, you'll do your own LDA in Pyro. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WAMmfJ51tMPt"
      },
      "source": [
        "We start by the usual imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EfN_fPI5AQ3n",
        "outputId": "97b1ae19-7ab6-40ff-a648-ad1b512c01bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "!pip install pyro-ppl"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: pyro-ppl in c:\\programdata\\anaconda3\\envs\\mbml\\lib\\site-packages (1.3.1)\nRequirement already satisfied: numpy>=1.7 in c:\\programdata\\anaconda3\\envs\\mbml\\lib\\site-packages (from pyro-ppl) (1.18.1)\nRequirement already satisfied: pyro-api>=0.1.1 in c:\\programdata\\anaconda3\\envs\\mbml\\lib\\site-packages (from pyro-ppl) (0.1.2)\nRequirement already satisfied: opt-einsum>=2.3.2 in c:\\programdata\\anaconda3\\envs\\mbml\\lib\\site-packages (from pyro-ppl) (3.2.1)\nRequirement already satisfied: tqdm>=4.36 in c:\\programdata\\anaconda3\\envs\\mbml\\lib\\site-packages (from pyro-ppl) (4.46.0)\nRequirement already satisfied: torch>=1.4.0 in c:\\programdata\\anaconda3\\envs\\mbml\\lib\\site-packages (from pyro-ppl) (1.5.0)\nRequirement already satisfied: future in c:\\programdata\\anaconda3\\envs\\mbml\\lib\\site-packages (from torch>=1.4.0->pyro-ppl) (0.18.2)\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JOPML4S8jiow",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.distributions import constraints\n",
        "import functools\n",
        "\n",
        "import pyro\n",
        "import pyro.distributions as dist\n",
        "from pyro.infer import SVI, JitTraceEnum_ELBO, TraceEnum_ELBO\n",
        "from pyro.contrib.autoguide import AutoDiagonalNormal, AutoMultivariateNormal, AutoGuideList, AutoDelta\n",
        "from pyro.optim import ClippedAdam"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oNHu_u01tMPu",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf2112ju4drc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests, io\n",
        "r = requests.get('https://github.com/MikkelGroenning/MBML_project/blob/master/data/processed/upsampled_data.npy?raw=true')\n",
        "\n",
        "data = np.load(io.BytesIO(r.content)).astype('int32')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yh986IBl4drk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_sub = data[:100]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRMyCvdZ75oL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_sub = np.vectorize({k:v for (k,v) in zip(np.unique(data_sub), np.arange(len(np.unique(data_sub))))}.get)(data_sub)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLGtFboQ4drw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_words = len(np.unique(data_sub))#data_sub.max()\n",
        "num_topics = 10\n",
        "num_docs = data_sub.shape[0]\n",
        "num_words_per_doc = data_sub.shape[1]\n",
        "\n",
        "def model(data=None, batch_size=None):\n",
        "    \"\"\" Make a plate of size num_topics with name \"topics\" and define a variable \"topic_words\".\n",
        "          This represents the phi above. Use the equivalent of a uniform distribution for it  \"\"\"\n",
        "    with pyro.plate(\"topics\", num_topics):\n",
        "        topic_words = pyro.sample(\"topic_words\", dist.Dirichlet(torch.ones(num_words) / num_words))\n",
        "\n",
        "    \"\"\" Make two (nested) plates in here. One over documents and one over words\n",
        "          Documents, called \"documents\":\n",
        "          The plate over the documents should hold a variable \"doc_topics\" representing the theta above.\n",
        "            Use the equivalent of a uniform distribution for it.\n",
        "          \n",
        "          Words, called \"words\":\n",
        "          The plate over words, should have a topic assignment for each word (z_{i,j} above) which \n",
        "            should be enumerated.\n",
        "          The second variable should be the words themselves which should be drawn from the \"topic_words\"\n",
        "            using the assigned z_{i,j} and the observed data.\n",
        "\n",
        "     \"\"\"\n",
        "    with pyro.plate(\"documents\", num_docs) as ind:\n",
        "        if data is not None:\n",
        "            with pyro.util.ignore_jit_warnings():\n",
        "                assert data.shape == (num_words_per_doc, num_docs)\n",
        "            data = data[:, ind]\n",
        "        doc_topics = pyro.sample(\"doc_topics\", dist.Dirichlet(torch.ones(num_topics)/ num_topics))\n",
        "        with pyro.plate(\"words\", num_words_per_doc):\n",
        "            # The word_topics variable is marginalized out during inference,\n",
        "            # achieved by specifying infer={\"enumerate\": \"parallel\"} and using\n",
        "            # TraceEnum_ELBO for inference. Thus we can ignore this variable in\n",
        "            # the guide.\n",
        "            word_topics = pyro.sample(\"word_topics\", dist.Categorical(doc_topics), infer={\"enumerate\": \"parallel\"})\n",
        "            data = pyro.sample(\"doc_words\", dist.Categorical(topic_words[word_topics]), obs=data)\n",
        "\n",
        "    return topic_words, data"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ThkcdOU4dry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W_torch = torch.tensor(data_sub.T).long()\n",
        "W_torch.shape\n",
        "del data"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1sGgWky5RnR",
        "colab_type": "code",
        "outputId": "9ded812e-58c4-44e9-efd0-84b23eb50194",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "(num_words_per_doc, num_docs)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "(1500, 100)"
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18EC73FB4dr3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "outputId": "b411b1ff-fdcf-44a2-eead-cdcc4d9f8f19"
      },
      "source": [
        "pyro.clear_param_store()\n",
        "\n",
        "def my_local_guide(data=None, batch_size=None):\n",
        "    topic_words_posterior = pyro.param(\n",
        "            \"topic_words_posterior\",\n",
        "            lambda: torch.ones(num_topics, num_words),\n",
        "            constraint=constraints.positive)\n",
        "    with pyro.plate(\"topics\", num_topics):\n",
        "        pyro.sample(\"topic_words\", dist.Dirichlet(topic_words_posterior))\n",
        "    \n",
        "    doc_topics_posterior = pyro.param(\n",
        "            \"doc_topics_posterior\",\n",
        "            lambda: torch.ones(num_docs, num_topics),\n",
        "            constraint=constraints.simplex)\n",
        "    with pyro.plate(\"documents\", num_docs, batch_size) as ind:\n",
        "        pyro.sample(\"doc_topics\", dist.Delta(doc_topics_posterior[ind], event_dim=1))\n",
        "    \n",
        "guide = AutoGuideList(model)\n",
        "guide.add(AutoDiagonalNormal(pyro.poutine.block(model, expose=['doc_topics'])))\n",
        "guide.add(my_local_guide)  # automatically wrapped in an AutoCallable\n",
        "\n",
        "guide = my_local_guide\n",
        "\n",
        "elbo = TraceEnum_ELBO(max_plate_nesting=3)\n",
        "\n",
        "optim = ClippedAdam({'lr': 0.05})\n",
        "svi = SVI(model, guide, optim, elbo)\n",
        "\n",
        "# Define the number of optimization steps\n",
        "n_steps = 750\n",
        "\n",
        "# do gradient steps\n",
        "for step in range(n_steps):\n",
        "    elbo = svi.step(W_torch, batch_size=16)\n",
        "    if step % 25 == 0:\n",
        "        #print('.', end='')\n",
        "        print(\"[%d] ELBO: %.1f\" % (step, elbo))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 3922560000 bytes. Buy new RAM!\n(no backtrace available)",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-10-47f809b7a531>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# do gradient steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0melbo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW_torch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m25\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m#print('.', end='')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\mbml\\lib\\site-packages\\pyro\\infer\\svi.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;31m# get loss and compute gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mpoutine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mparam_capture\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_and_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mguide\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         params = set(site[\"value\"].unconstrained()\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\mbml\\lib\\site-packages\\pyro\\infer\\traceenum_elbo.py\u001b[0m in \u001b[0;36mloss_and_grads\u001b[1;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[0;32m    414\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtrainable_params\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0melbo_particle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m                 \u001b[0mloss_particle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0melbo_particle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m                 \u001b[1;33m(\u001b[0m\u001b[0mloss_particle\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_particles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0melbo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\mbml\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\mbml\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 3922560000 bytes. Buy new RAM!\n(no backtrace available)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mWQc-vc3tMPy"
      },
      "source": [
        "## Part 1 - Understanding the dirichlet distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vOgXLE9DtMPz"
      },
      "source": [
        "The Dirichlet distribution is available as numpy.random.dirichlet(alpha, size=None)\n",
        "\n",
        "...so, try it! For example, obtain draws from this distribution using different values of alpha."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C6i56CjAtMPz",
        "outputId": "e7ef13bc-e352-44bd-dfe7-9f36346cb21b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(np.random.dirichlet([.2,.2, .2]))\n",
        "print(np.random.dirichlet([.1,.1, .9]))\n",
        "print(np.random.dirichlet([1,1, 1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.03045048 0.46723908 0.50231044]\n",
            "[0.00127637 0.50272485 0.49599878]\n",
            "[0.30965595 0.28939292 0.40095113]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7Bx-u8OZtMP3"
      },
      "source": [
        "Check that the sum is always 1, for all vectors..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JAm0vYGmtMP4",
        "outputId": "7279acf7-4555-44e1-8ea1-6f48a62b75c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.sum(np.random.dirichlet([.2,.2, .2]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9999999999999999"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3NBgYmnTtMP7"
      },
      "source": [
        "Whenever you can, try to visualize it. Remember what we did in the slides. Try to do the same thing!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y_bCddBStMP8"
      },
      "source": [
        "**feel free to use the function below, to plot points from a dirichlet distribution, onto a 2D simplex**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OyTZBnJZtMP9",
        "colab": {}
      },
      "source": [
        "'''Function to plot points in a simplex'''\n",
        "\n",
        "# Based on post from Thomas Boggs (http://blog.bogatron.net/blog/2014/02/02/visualizing-dirichlet-distributions/)\n",
        "\n",
        "import matplotlib.tri as tri\n",
        "\n",
        "_corners = np.array([[0, 0], [1, 0], [0.5, 0.75**0.5]])\n",
        "_triangle = tri.Triangulation(_corners[:, 0], _corners[:, 1])\n",
        "\n",
        "def plot_points(X):\n",
        "    '''Plots a set of points in the simplex.\n",
        "\n",
        "    Arguments:\n",
        "\n",
        "        `X` (ndarray): A 2xN array (if in Cartesian coords) or 3xN array\n",
        "                       (if in barycentric coords) of points to plot.\n",
        "    '''\n",
        "    \n",
        "    X = X.dot(_corners)  #This is what converts the original points onto the simplex (it projects on it, through dot product)\n",
        "    plt.plot(X[:, 0], X[:, 1], 'k.', ms=1)\n",
        "    plt.axis('equal')\n",
        "    plt.xlim(0, 1)\n",
        "    plt.ylim(0, 0.75**0.5)\n",
        "    plt.axis('off')\n",
        "    plt.triplot(_triangle, linewidth=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cRWdwC18tMQA"
      },
      "source": [
        "Generate 10000 points from the dirichlet distribution and plot them using the function above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mqrhlkQ2tMQB",
        "colab": {}
      },
      "source": [
        "points = np.random.dirichlet([.09, .09, .09], size=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GZLY_VrTtMQD",
        "outputId": "a41fc666-ee33-4be0-8942-ae3bc2ea3a18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "plot_points(points)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADnCAYAAAC9roUQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSrqEEBJC/w8VuLLzGeZt/wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V1MgMP8RtMQG"
      },
      "source": [
        "Try with different values of $\\alpha$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e-XevlM5tMQH"
      },
      "source": [
        "Notice that $\\alpha = [1,1,1]$ corresponds to a uniform distribution. Values of $\\alpha < 1$ lead to sparse distributions (can be used as sparsity-inducing priors!). The higher the value of $\\alpha$, the more concentrated is the distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V3BduTcBtMQH"
      },
      "source": [
        "----------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R_XILq03tMQI"
      },
      "source": [
        "## Part 2: LDA Ancestral Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aKg0OaRIkq6s",
        "outputId": "d27cd983-ef0f-4149-fcf2-128896dd1715",
        "colab": {}
      },
      "source": [
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/C-8hgB-JQl4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/C-8hgB-JQl4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2-2tBrxdtMQJ"
      },
      "source": [
        "We define our own dictionary of 30 words. There are 3 different topics that we embedded in the dictionary, can you see what they are?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cCs9PhNGtMQK",
        "colab": {}
      },
      "source": [
        "K = 3\n",
        "dictionary =[\"Copenhagen\",\n",
        "             \"Madrid\",\n",
        "             \"Sydney\",\n",
        "             \"Kabul\",\n",
        "             \"Vienna\",\n",
        "             \"Brussels\",\n",
        "             \"Beijing\",\n",
        "             \"Kathmandu\",\n",
        "             \"Singapore\",\n",
        "             \"Oslo\",\n",
        "             \"blue\",\n",
        "             \"green\",\n",
        "             \"beige\",\n",
        "             \"cyan\",\n",
        "             \"black\",\n",
        "             \"tan\",\n",
        "             \"brown\",\n",
        "             \"orange\",\n",
        "             \"white\",\n",
        "             \"red\",\n",
        "             \"data\",\n",
        "             \"model\",\n",
        "             \"inference\",\n",
        "             \"learning\",\n",
        "             \"observation\",\n",
        "             \"dimension\",\n",
        "             \"training\",\n",
        "             \"neuralnetwork\",\n",
        "             \"analytics\",\n",
        "             \"sampling\"\n",
        "             ]\n",
        "C = len(dictionary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1AteOLf-tMQM"
      },
      "source": [
        "To make it smoother and more fun, we'll start by giving you the topics, $\\phi$ and the proportions $\\theta$ directly. I.e., you don't need to generate (yet) those from Dirichlet distribution.\n",
        "\n",
        "Lets start by defining the 3 topics. We do this by assigning a probability for each word under of the 3 topics ($\\phi$)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2kjd8bZttMQN",
        "colab": {}
      },
      "source": [
        "## Define word vectors (for each topic) and normalize:\n",
        "phi = np.zeros( (K, C) );\n",
        "phi[0] = [0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9,\n",
        "          0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
        "          0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
        "phi[0] *= 1/np.sum(phi[0])\n",
        "phi[1] = [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
        "          0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9,\n",
        "          0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
        "phi[1] *= 1/np.sum(phi[1])\n",
        "phi[2] = [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
        "          0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
        "          0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9]\n",
        "phi[2] *= 1/np.sum(phi[2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tkfjtCqotMQP"
      },
      "source": [
        "Notice that for topic 1, we assigned high probability to the words: Copenhagen, Madrid, Sydney, Kabul, Vienna, Brussels, Beijing, Kathmandu, Singapore, Oslo. \n",
        "\n",
        "Similarly, for topic 2, we assigned high probability to the words: blue, green, ...\n",
        "\n",
        "Finaly, for topic 3, we assigned high probability to the words: model, inference, ...\n",
        "\n",
        "Now that we have defined the probability of each word under each topic, we can follow the generative process to sample some documents. As previously mentioned, we will pre-define the distribution over topics for each document ($\\theta$):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nZT2W-4NtMQQ",
        "colab": {}
      },
      "source": [
        "I = 3 ## Number of documents.\n",
        "\n",
        "## Define topic proportion vectors (one for for each document):\n",
        "theta = np.zeros( (I, K) );\n",
        "theta[0] = [0.6, 0.2, 0.2]\n",
        "theta[1] = [1/3, 1/3, 1/3] ## This one will be an equal mix of all topics\n",
        "theta[2] = [  0,   0,  1] ## This will for example only contain topic 3."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lAWNtsn7tMQS"
      },
      "source": [
        "Notice that the first document has higher probability of containing words from topic 1 than topics 2 and 3. Document 2 is uniform - all topics have equal probability. Document 3 covers exclusevely topic 3.\n",
        "\n",
        "Let us now generate the 3 documents following the generative process:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-Bp7PwHXtMQT"
      },
      "source": [
        "So, you assume now that we have $\\boldsymbol{\\theta}_i$ for all documents and $\\boldsymbol{\\phi}_k$ for all topics. Ancestral sampling is simply:\n",
        "\n",
        "\n",
        "For each document $i$, and for each word $j=1...w_i$, do:\n",
        "\\begin{align}\n",
        "z_{i, j} &\\sim Cat(\\boldsymbol{\\theta}_i) \\\\\n",
        "w_{i, j} &\\sim Cat(\\boldsymbol{\\phi}_{z_{i,j}})\n",
        "\\end{align}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GhDeBaV-tMQU"
      },
      "source": [
        "Can you try code it down yourself?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "51S3pRistMQV",
        "colab": {}
      },
      "source": [
        "##Just to make things practical, we created this function for you\n",
        "## just read it or, try it, to see what it does...\n",
        "def categorical_sample(p):\n",
        "    return list(np.random.multinomial(1, p)).index(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4GSTTn28tMQX",
        "colab": {}
      },
      "source": [
        "## function that receives document sizes J, theta (topic proportions) and phi (topics), \n",
        "## returns z (word-topic assignments ) and w (words)\n",
        "def ancestral_sampling(J, theta, phi):\n",
        "    ## Initialise\n",
        "    z = np.zeros( (I, np.max(J)), dtype=int )  #NOTICE that z and w are vectors of integers!\n",
        "    w = np.zeros( (I, np.max(J)), dtype=int )  \n",
        "\n",
        "    for i in range(I):\n",
        "        for j in range(J[i]):\n",
        "            z[i,j] = categorical_sample(theta[i])\n",
        "            w[i,j] = categorical_sample(phi[z[i,j]])\n",
        "    return z, w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VtahF-iktMQZ",
        "outputId": "0e963e85-5069-4f19-b143-eb4c8003b92d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "J = [12, 9, 10] ## Vector of size I denoting how many words are in each document\n",
        "z, w = ancestral_sampling(J, theta, phi)\n",
        "print(\"w:\", w)\n",
        "for i in range(I):\n",
        "    print(\"\\n\\nDocument: \", i)\n",
        "    print(\"Theta: \", theta[i])\n",
        "    for j in range(J[i]):\n",
        "        print(\"Word %d: Topic assignment: %d->   %s  \" % (j,  z[i, j]+1, dictionary[w[i, j]]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w: [[ 7  0 20  6  3  8 27  9  3  6  3  2]\n",
            " [12 18 28 25 24 14 21 20 13  0  0  0]\n",
            " [20 22 28 21 29 25 27 24  9 20  0  0]]\n",
            "\n",
            "\n",
            "Document:  0\n",
            "Theta:  [0.6 0.2 0.2]\n",
            "Word 0: Topic assignment: 1->   Kathmandu  \n",
            "Word 1: Topic assignment: 1->   Copenhagen  \n",
            "Word 2: Topic assignment: 3->   data  \n",
            "Word 3: Topic assignment: 2->   Beijing  \n",
            "Word 4: Topic assignment: 1->   Kabul  \n",
            "Word 5: Topic assignment: 1->   Singapore  \n",
            "Word 6: Topic assignment: 3->   neuralnetwork  \n",
            "Word 7: Topic assignment: 1->   Oslo  \n",
            "Word 8: Topic assignment: 1->   Kabul  \n",
            "Word 9: Topic assignment: 1->   Beijing  \n",
            "Word 10: Topic assignment: 1->   Kabul  \n",
            "Word 11: Topic assignment: 1->   Sydney  \n",
            "\n",
            "\n",
            "Document:  1\n",
            "Theta:  [0.33333333 0.33333333 0.33333333]\n",
            "Word 0: Topic assignment: 2->   beige  \n",
            "Word 1: Topic assignment: 3->   white  \n",
            "Word 2: Topic assignment: 3->   analytics  \n",
            "Word 3: Topic assignment: 3->   dimension  \n",
            "Word 4: Topic assignment: 3->   observation  \n",
            "Word 5: Topic assignment: 2->   black  \n",
            "Word 6: Topic assignment: 3->   model  \n",
            "Word 7: Topic assignment: 3->   data  \n",
            "Word 8: Topic assignment: 2->   cyan  \n",
            "\n",
            "\n",
            "Document:  2\n",
            "Theta:  [0. 0. 1.]\n",
            "Word 0: Topic assignment: 3->   data  \n",
            "Word 1: Topic assignment: 3->   inference  \n",
            "Word 2: Topic assignment: 3->   analytics  \n",
            "Word 3: Topic assignment: 3->   model  \n",
            "Word 4: Topic assignment: 3->   sampling  \n",
            "Word 5: Topic assignment: 3->   dimension  \n",
            "Word 6: Topic assignment: 3->   neuralnetwork  \n",
            "Word 7: Topic assignment: 3->   observation  \n",
            "Word 8: Topic assignment: 3->   Oslo  \n",
            "Word 9: Topic assignment: 3->   data  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Jd4kdRTftMQc"
      },
      "source": [
        "Don't forget to take a look at the generated documents? Do they make sense, given the values for $\\phi$ and $\\theta$ given above?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H4vBO1wDtMQd"
      },
      "source": [
        "We bet you guessed what we want you to do next: having generated yourself the data, can you make an LDA model in Pyro that recovers the original parameters $\\phi$ and $\\theta$?\n",
        "\n",
        "A dataset with only 3 documents is too little of course. So, let's generate 100 documents intead, by using the dirichlet distribution. In other words, we will generate:\n",
        "- vectors $\\theta_1 \\dots \\theta_I$\n",
        "- a new vector $J$ that contains their size (10)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q6cDzjJ5tMQd",
        "colab": {}
      },
      "source": [
        "I=100\n",
        "theta = np.zeros((I, K));\n",
        "alpha = 0.5*np.ones(K) ## Size K dirichlet prior\n",
        "J=[]\n",
        "\n",
        "for i in range(I):\n",
        "    theta[i] = np.random.dirichlet(alpha);\n",
        "    J.append(10)\n",
        "_, W = ancestral_sampling(J, theta, phi)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DwMY0L_NtMQg"
      },
      "source": [
        "Please reuse your code to generate the dataset W. Please reuse $\\phi$ as before"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a8wSPNmUtMQh"
      },
      "source": [
        "## Part 3 - Implement your own LDA model in Pyro"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9YwqrfkRkq7A",
        "outputId": "1a19270c-b7da-4962-ffe3-b3d0285494a5",
        "colab": {}
      },
      "source": [
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/wbRkLBgEfQg\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/wbRkLBgEfQg\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_hXAqX-_j8_w"
      },
      "source": [
        "Let's try to implement  the model in Pyro. In order to implement the model we will need  the ```Dirichlet(alpha)``` and ```Categorical(pi)```  distributions.  We also need to marginalize the \"word_topic\" distribution as we did in the mixture model notebooks using ```infer={\"enumerate\": \"parallel\"}``` inside the distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "skhrztWZlV-d"
      },
      "source": [
        "For a short recap, the generative process is the following:\n",
        "\n",
        "\\begin{align}\n",
        "\\phi_{k} & \\sim Dirichlet(\\alpha) \\quad \\textrm{Distribution of words in topic} \\\\\n",
        "\\theta_{i} & \\sim Dirichlet(\\beta) \\quad \\textrm{Distribution of topics in document} \\\\\n",
        "& z_{i,j} \\sim Categorical(\\theta_{i}) \\quad \\textrm{A topic assignment to word $j$ in document $i$}\\\\\n",
        "& w_{i,j}  \\sim Categorical(\\phi_{z_{i,j}}) \\quad \\textrm{A draw of word $j$ in document $i$}  \\\\\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l4P8D-EZFQ9W"
      },
      "source": [
        "__NOTE! make sure to use the names we propose in the commented text inside the model. This is important for the inference part.__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AM2P5mcftMQl",
        "colab": {}
      },
      "source": [
        "# This is a fully generative model of a batch of documents.\n",
        "# data is a [num_words_per_doc, num_documents] shaped array of word ids\n",
        "# (specifically it is not a histogram). We assume in this simple example\n",
        "# that all documents have the same number of words.\n",
        "\n",
        "num_words = C\n",
        "num_topics = K\n",
        "num_docs = I\n",
        "num_words_per_doc = 10\n",
        "\n",
        "def model(data=None, batch_size=None):\n",
        "    \"\"\" Make a plate of size num_topics with name \"topics\" and define a variable \"topic_words\".\n",
        "          This represents the phi above. Use the equivalent of a uniform distribution for it  \"\"\"\n",
        "    with pyro.plate(\"topics\", num_topics):\n",
        "        topic_words = pyro.sample(\"topic_words\", dist.Dirichlet(torch.ones(num_words) / num_words))\n",
        "\n",
        "    \"\"\" Make two (nested) plates in here. One over documents and one over words\n",
        "          Documents, called \"documents\":\n",
        "          The plate over the documents should hold a variable \"doc_topics\" representing the theta above.\n",
        "            Use the equivalent of a uniform distribution for it.\n",
        "          \n",
        "          Words, called \"words\":\n",
        "          The plate over words, should have a topic assignment for each word (z_{i,j} above) which \n",
        "            should be enumerated.\n",
        "          The second variable should be the words themselves which should be drawn from the \"topic_words\"\n",
        "            using the assigned z_{i,j} and the observed data.\n",
        "\n",
        "     \"\"\"\n",
        "    with pyro.plate(\"documents\", num_docs) as ind:\n",
        "        if data is not None:\n",
        "            with pyro.util.ignore_jit_warnings():\n",
        "                assert data.shape == (num_words_per_doc, num_docs)\n",
        "            data = data[:, ind]\n",
        "        doc_topics = pyro.sample(\"doc_topics\", dist.Dirichlet(torch.ones(num_topics)/ num_topics))\n",
        "        with pyro.plate(\"words\", num_words_per_doc):\n",
        "            # The word_topics variable is marginalized out during inference,\n",
        "            # achieved by specifying infer={\"enumerate\": \"parallel\"} and using\n",
        "            # TraceEnum_ELBO for inference. Thus we can ignore this variable in\n",
        "            # the guide.\n",
        "            word_topics = pyro.sample(\"word_topics\", dist.Categorical(doc_topics), infer={\"enumerate\": \"parallel\"})\n",
        "            data = pyro.sample(\"doc_words\", dist.Categorical(topic_words[word_topics]), obs=data)\n",
        "\n",
        "    return topic_words, data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "COQllexBtMQn",
        "colab": {}
      },
      "source": [
        "# We can generate synthetic data directly by calling the model.\n",
        "#true_topic_words, fake_data = model()\n",
        "#fake_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JjY_-FkktMQp",
        "outputId": "e74c1568-bf33-48dc-b569-f848487efb5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "W_torch = torch.tensor(W.T).long()\n",
        "W_torch.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5ngYkQeztMQr"
      },
      "source": [
        "## Manual+Autoguide version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6qdrVyuUGvvD"
      },
      "source": [
        "In Variational Inference (VI) we define a \"guide\", a function similar to a model definition where we declare, for each distribution in the model, an approximating distribution \"easy\" to optimize. In this case, the inference can't be done using the \"autoguide\" as we have done in previous notebooks. The problem is that autoguide is using a multivariate normal distribution to approximate all the distributions in the model. However, some distributions (like a dirichlet) cannot be approximated using a normal. We have to use a \"manual\" guide in here.\n",
        "\n",
        "Don't worry too much about this as we will learn the details on the variational inference lecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w7gOCIdstMQs",
        "outputId": "3926cda0-d475-4a60-afba-e0377f494f57",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "pyro.clear_param_store()\n",
        "\n",
        "def my_local_guide(data=None, batch_size=None):\n",
        "    topic_words_posterior = pyro.param(\n",
        "            \"topic_words_posterior\",\n",
        "            lambda: torch.ones(num_topics, num_words),\n",
        "            constraint=constraints.positive)\n",
        "    with pyro.plate(\"topics\", num_topics):\n",
        "        pyro.sample(\"topic_words\", dist.Dirichlet(topic_words_posterior))\n",
        "    \n",
        "    doc_topics_posterior = pyro.param(\n",
        "            \"doc_topics_posterior\",\n",
        "            lambda: torch.ones(num_docs, num_topics),\n",
        "            constraint=constraints.simplex)\n",
        "    with pyro.plate(\"documents\", num_docs, batch_size) as ind:\n",
        "        pyro.sample(\"doc_topics\", dist.Delta(doc_topics_posterior[ind], event_dim=1))\n",
        "    \n",
        "guide = AutoGuideList(model)\n",
        "guide.add(AutoDiagonalNormal(pyro.poutine.block(model, expose=['doc_topics'])))\n",
        "guide.add(my_local_guide)  # automatically wrapped in an AutoCallable\n",
        "\n",
        "guide = my_local_guide\n",
        "\n",
        "elbo = TraceEnum_ELBO(max_plate_nesting=3)\n",
        "\n",
        "optim = ClippedAdam({'lr': 0.005})\n",
        "svi = SVI(model, guide, optim, elbo)\n",
        "\n",
        "# Define the number of optimization steps\n",
        "n_steps = 4000\n",
        "\n",
        "# do gradient steps\n",
        "for step in range(n_steps):\n",
        "    elbo = svi.step(W_torch, batch_size=32)\n",
        "    if step % 100 == 0:\n",
        "        #print('.', end='')\n",
        "        print(\"[%d] ELBO: %.1f\" % (step, elbo))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0] ELBO: 3831.8\n",
            "[100] ELBO: 3794.2\n",
            "[200] ELBO: 3800.8\n",
            "[300] ELBO: 3748.8\n",
            "[400] ELBO: 3773.2\n",
            "[500] ELBO: 3747.5\n",
            "[600] ELBO: 3653.7\n",
            "[700] ELBO: 3704.8\n",
            "[800] ELBO: 3652.2\n",
            "[900] ELBO: 3560.3\n",
            "[1000] ELBO: 3266.5\n",
            "[1100] ELBO: 3241.8\n",
            "[1200] ELBO: 3204.7\n",
            "[1300] ELBO: 3159.2\n",
            "[1400] ELBO: 3028.6\n",
            "[1500] ELBO: 3170.4\n",
            "[1600] ELBO: 3004.5\n",
            "[1700] ELBO: 2921.1\n",
            "[1800] ELBO: 2828.6\n",
            "[1900] ELBO: 2819.6\n",
            "[2000] ELBO: 2662.1\n",
            "[2100] ELBO: 2825.5\n",
            "[2200] ELBO: 2845.1\n",
            "[2300] ELBO: 2769.1\n",
            "[2400] ELBO: 2610.5\n",
            "[2500] ELBO: 2513.4\n",
            "[2600] ELBO: 2579.7\n",
            "[2700] ELBO: 2385.1\n",
            "[2800] ELBO: 2250.6\n",
            "[2900] ELBO: 2225.3\n",
            "[3000] ELBO: 2332.6\n",
            "[3100] ELBO: 2608.5\n",
            "[3200] ELBO: 2137.6\n",
            "[3300] ELBO: 2462.3\n",
            "[3400] ELBO: 2077.7\n",
            "[3500] ELBO: 2097.7\n",
            "[3600] ELBO: 2219.5\n",
            "[3700] ELBO: 1999.6\n",
            "[3800] ELBO: 2243.5\n",
            "[3900] ELBO: 1844.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TOJ_nWwBIFbq"
      },
      "source": [
        "Compare estimated $\\hat{\\phi}$ with true values of $\\phi$ used to generate the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Oej01OZ8tMRC",
        "outputId": "cf51db08-7239-4d3f-ca37-fd8235e251e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from pyro.infer import Predictive\n",
        "\n",
        "predictive = Predictive(model, guide=guide, num_samples=800,\n",
        "                        return_sites=(\"topic_words\",))\n",
        "samples = predictive(W_torch)\n",
        "samples[\"topic_words\"].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([800, 1, 5, 1213])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uI1rTzq4tMRF",
        "outputId": "32bffa86-f4f3-41f5-ddc9-f395b8f410e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Compute the argmax of the mean of all samples of topic for each word\n",
        "#   this is the most likely topic for each word in our dictionary\n",
        "samples[\"topic_words\"].mean(axis=0)[0].argmax(axis=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4, 4, 4,  ..., 0, 3, 2], grad_fn=<NotImplemented>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-4B2FZCMtMRH",
        "outputId": "94a9fee8-fbbc-4490-ed11-6a0a6662d2c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Now compute the same for the real values\n",
        "np.argmax(phi, axis=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NEuXtZMftMSU"
      },
      "source": [
        "Notice anything interesting? Do the results match exactly? Remember that if we call \"topic 1\" -> \"topic 2\", and we call \"topic 2\" -> \"topic 1\", the resulting model is still the same! We just \"renamed\" the topics. This is another case of the problem of model identifiability...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yNXjjNSKIkIG"
      },
      "source": [
        "Now let's compare the estimated $\\hat{\\theta}_{i}$ with true values of $\\theta_{i}$ used to generate the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ABv1cUDFtMQv",
        "colab": {}
      },
      "source": [
        "from pyro.infer import Predictive\n",
        "\n",
        "predictive = Predictive(model, guide=guide, num_samples=800,\n",
        "                        return_sites=(\"doc_topics\",))\n",
        "predictive = Predictive(model, guide=guide, num_samples=800)\n",
        "samples = predictive(W_torch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Yq9TLZaEAeAX",
        "outputId": "a3651f48-45e4-4958-a463-bc9e1144a30c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "samples.keys()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['topic_words', 'doc_topics', 'word_topics', 'doc_words'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z_vrkziltMQx",
        "outputId": "995858b9-1cc7-481a-bc55-3ffe2e60aed1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "doc_topics_posterior = samples[\"doc_topics\"].mean(axis=0)\n",
        "doc_topics_posterior.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 25, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BUdP6q8mtMQ0",
        "outputId": "15030049-2ce4-42ce-ac15-27180a624c42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Compute the argmax of the mean of all samples of topic for each document\n",
        "#   this is the most likely topic for each document in our data\n",
        "torch.argmax(doc_topics_posterior, axis=2)[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4, 4, 2, 1, 1, 4, 4, 0, 4, 4, 3, 3, 3, 3, 0, 2, 4, 4, 0, 2, 0, 1, 4, 1,\n",
              "        1], grad_fn=<SelectBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "36WIRN-NtMQ3",
        "outputId": "5271b4ff-7a83-4610-b860-f13dcf457afe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Compute the same quantity for the real values\n",
        "np.argmax(theta, axis=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 0, 1, 0, 2, 0, 1, 2, 2, 2, 2, 0, 2, 2, 0, 1, 1, 1, 1, 2, 2, 1,\n",
              "       2, 0, 2, 0, 2, 2, 0, 0, 2, 0, 1, 2, 0, 1, 1, 0, 1, 2, 2, 2, 2, 0,\n",
              "       1, 1, 1, 2, 1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,\n",
              "       2, 2, 2, 0, 2, 0, 2, 0, 2, 1, 1, 0, 1, 2, 0, 2, 0, 1, 0, 2, 2, 1,\n",
              "       1, 0, 2, 2, 0, 1, 0, 0, 1, 1, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "08 - LDA - Pyro - solutions.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "mWQc-vc3tMPy"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3.7.7 64-bit ('mbml': conda)",
      "language": "python",
      "name": "python37764bitmbmlcondafd08ce0341164d10a999be67eed30d97"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7-final"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}